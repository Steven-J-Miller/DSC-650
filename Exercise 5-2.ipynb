{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steven Miller\n",
    "### DSC 650 Winter 2020\n",
    "### 2020-01-08\n",
    "\n",
    "#### Exercise 5.2: Create a Small Data Warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "warehouse_dir = '/home/steven/Documents/DSC 650 Week 5/data'\n",
    "import os\n",
    "\n",
    "spark = SparkSession.builder.appName('Exercise5').config(\"spark.sql.warehouse.dir\", warehouse_dir).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1. Gazetteer Data**\n",
    "\n",
    "**a. Create Unmanaged Tables**\n",
    "\n",
    "The first step of this assignment involves loading the data from the CSV files, combining the file with the file for the other year, and saving it to disk as a table. The following code should provide a template to help you combine tables and save them to the warehouse directory. Click on the image to download the sample code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = [_[:-4] for _ in os.listdir('gazetteer/2018/')]\n",
    "\n",
    "for table in tables:\n",
    "    csv_file_path1 = f'gazetteer/2017/{table}.csv'\n",
    "    csv_file_path2 = f'gazetteer/2018/{table}.csv'\n",
    "\n",
    "    df1 = spark.read.load(csv_file_path1, format='csv', sep=',', inferSchema=True, header=True)\n",
    "    df2 = spark.read.load(csv_file_path2, format='csv', sep=',', inferSchema=True, header=True)\n",
    "\n",
    "    df = df1.unionAll(df2)\n",
    "    df.write.saveAsTable(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each CSV file in the 2017 and 2018 directories, load the data into Spark, combine it with the corresponding data from the other year and save it to disk. Once you have finished saving all of the files as tables, verify that you have loaded the files properly by loading the tables into Spark, and performing a simple row count on each table.\n",
    "\n",
    "The following Python code should provide you a template for loading the tables as an external table in Spark. Click on the image to download the sample code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_external_table(table_name):\n",
    "    table_dir = os.path.join(warehouse_dir, table_name)\n",
    "    return spark.catalog.createExternalTable(table_name, table_dir)\n",
    "\n",
    "def create_external_tables():\n",
    "    for table_name in tables:\n",
    "        create_external_table(table_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated previously, in a typical Hadoop distribution, you could save these tables as persistent tables in Apache Hive, but since we are not introducing Hive in this class, we need to load these tables into Spark and query them using SQL within Python.\n",
    "\n",
    "The following code shows how to count the number of rows in the places table and show the results. Click on the image to download the code.\n",
    "\n",
    "As an aside, spark.catalog module offers useful utility functions such as spark.catalog.listTables() to list all of the currently available tables. These are useful for inspecting the Spark SQL warehouse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|row_count|\n",
      "+---------+\n",
      "|    59151|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT COUNT(*) AS row_count FROM places\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b. Load and Query Tables**\n",
    "\n",
    "Now that we have saved the data to external tables, we will load the tables back into Spark and create a report using Spark SQL. For this report, we will create a report on school districts for the states of Nebraska and Iowa using the elementary_schools, secondary_schools and unified_school_districts tables. Using Spark SQL, create a report with the following information.\n",
    "\n",
    "This table contains the number of elementary, secondary, and unified school districts in each state for each year. Note that the numbers in this table are notional and do not represent the actual results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----------+---------+-------+\n",
      "|state|year|Elementary|Secondary|Unified|\n",
      "+-----+----+----------+---------+-------+\n",
      "|   IA|2017|         0|        0|    336|\n",
      "|   IA|2018|         0|        0|    333|\n",
      "|   NE|2017|         0|        0|    251|\n",
      "|   NE|2018|         0|        0|    246|\n",
      "+-----+----+----------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT u.state, u.year, e.Elementary, s.Secondary, u.Unified  FROM \n",
    "                  (SELECT state, year, COUNT(name) as Unified\n",
    "                  FROM unified_school_districts\n",
    "                  GROUP BY state, year) u\n",
    "                  FULL OUTER JOIN\n",
    "                  (SELECT state, year, COUNT(name) as Elementary\n",
    "                  FROM elementary_schools\n",
    "                  GROUP BY state, year) e ON u.state=e.state AND u.year=e.year\n",
    "                  FULL OUTER JOIN\n",
    "                  (SELECT state, year, COUNT(name) as Secondary\n",
    "                  FROM secondary_schools\n",
    "                  GROUP BY state, year) s ON u.state=s.state AND e.state=s.state AND u.year=s.year AND e.year=s.year\n",
    "              WHERE u.state='NE' OR u.state='IA'\n",
    "              ORDER BY u.state, u.year\n",
    "              \"\"\").na.fill(0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2. Flight Data**\n",
    "\n",
    "In the previous exercise, you joined data from flights and airport codes to create a report. Create an external table for airport_codes and domestic_flights from the domestic-flights/flights.parquet and airport-codes/airport-codes.csv files. Recreate the report of top ten airports for 2008 using Spark SQL instead of dataframes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flights = spark.read.parquet('domestic-flights/flights.parquet')\n",
    "df_airport_codes = spark.read.load('airport-codes/airport-codes.csv', format='csv', sep=',', inferSchema=True, header=True)\n",
    "df_flights.write.saveAsTable('flights')\n",
    "df_airport_codes.write.saveAsTable('airport_codes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+---------+----------------+----------------+-------------+-------------+\n",
      "|Rank|        Airport Name|IATA Code|Total Passengers|Daily Passengers|Total Flights|Daily Flights|\n",
      "+----+--------------------+---------+----------------+----------------+-------------+-------------+\n",
      "|   1|Hartsfield Jackso...|      ATL|        35561795|           97429|       395192|         1082|\n",
      "|   2|Chicago O'Hare In...|      ORD|        26398793|           72325|       356570|          976|\n",
      "|   3|Dallas Fort Worth...|      DFW|        22883558|           62694|       270243|          740|\n",
      "|   4|Los Angeles Inter...|      LAX|        19741782|           54087|       215000|          589|\n",
      "|   5|McCarran Internat...|      LAS|        18262263|           50033|       164123|          449|\n",
      "|   6|Phoenix Sky Harbo...|      PHX|        17305718|           47412|       181259|          496|\n",
      "|   7|Charlotte Douglas...|      CLT|        15038489|           41201|       205040|          561|\n",
      "|   8|George Bush Inter...|      IAH|        14870717|           40741|       214245|          586|\n",
      "|   9|Orlando Internati...|      MCO|        14581086|           39948|       131710|          360|\n",
      "|  10|Detroit Metropoli...|      DTW|        14228887|           38983|       191910|          525|\n",
      "+----+--------------------+---------+----------------+----------------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''SELECT RANK() OVER (ORDER BY subtable.total_passengers DESC) as `Rank`, a.name as `Airport Name`, a.iata_code as `IATA Code`, \n",
    "            subtable.total_passengers as `Total Passengers`, INT(subtable.daily_passengers) as `Daily Passengers`, subtable.total_flights as `Total Flights`, \n",
    "            INT(subtable.daily_flights)  as `Daily Flights` FROM airport_codes a\n",
    "            INNER JOIN\n",
    "            (SELECT f.destination_airport_code as dest_code, SUM(f.passengers) as total_passengers, SUM(f.passengers)/365 as daily_passengers, SUM(f.flights) as total_flights, SUM(f.flights)/365 as daily_flights\n",
    "            FROM flights f\n",
    "            LEFT OUTER JOIN airport_codes d ON d.iata_code = f.destination_airport_code\n",
    "            WHERE f.flight_year = 2008\n",
    "            GROUP BY f.destination_airport_code) subtable\n",
    "            ON a.iata_code = subtable.dest_code\n",
    "            ORDER BY total_passengers DESC\n",
    "            LIMIT 10\n",
    "            ''').show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
